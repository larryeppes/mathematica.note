卷积神经网络
经典模型整理
字数
首先是再次对卷积神经网络的介绍
更深入的理解
卷积神经网络是人工神经网络的一种
已成为当前语音分析和图像识别领域的研究热点
它的权值共享网络结构使之更类似于生物神经网络
降低了网络模型的复杂度
减少了权值的数量
该优点在网络的输入是多维图像时表现的更为明显
使图像可以直接作为网络的输入
避免了传统识别
算法
中复杂的特征提取和数据重建过程
卷积网络是为识别二维形状而特殊设计的一个多层感知器
这种网络结构对平移
比例缩放
倾斜或者共他形式的变形具有高度不变性
是第一个真正成功训练多层网络结构的学习算法
它利用空间关系减少需要学习的参数数目以提高一般前向
算法的训练性能
作为一个深度学习
架构
提出是为了最小化数据的预处理要求
关于参数减少与权值共享
上面聊到
好像
一个牛逼的地方就在于通过感受野和权值共享减少了神经网络需要训练的参数的个数
那究竟是啥的呢
下图左
如果我们有
像素的图像
有
百万个隐层神经元
那么他们全连接的话
每个隐层神经元都连接图像的每一个像素点
就有
个连接
也就是
个权值参数
然而图像的空间联系是局部的
就像人是通过一个局部的感受野去感受外界图像一样
每一个神经元都不需要对全局图像做感受
每个神经元只感受局部的图像区域
然后在更高层
将这些感受不同局部的神经元综合起来就可以得到全局的信息了
这样
我们就可以减少连接的数目
也就是减少神经网络需要训练的权值参数的个数了
如下图右
假如局部感受野是
隐层每个感受野只需要和这
的局部图像相连接
所以
百万个隐层神经元就只有一亿个连接
即
个参数
比原来减少了四个
数量级
这样训练起来就没那么费力了
但还是感觉很多的啊
那还有啥办法没
我们知道
隐含层的每一个神经元都连接
个图像区域
也就是说每一个神经元存在
个连接权值参数
那如果我们每个神经元这
个参数是相同的呢
也就是说每个神经元用的是同一个卷积核去卷积图像
这样我们就只有多少个参数
只有
个参数啊
亲
不管你隐层的神经元个数有多少
两层间的连接我只有
个参数啊
亲
这就是权值共享啊
亲
这就是卷积神经网络的主打卖点啊
亲
有点烦了
呵呵
也许你会问
这样做靠谱吗
为什么可行呢
这个
共同学习
好了
你就会想
这样提取特征也忒不靠谱吧
这样你只提取了一种特征
一个卷积核对应一种特征
啊
对了
真聪明
我们需要提取多种特征对不
假如一种滤波器
也就是一种卷积核就是提出图像的一种特征
例如某个方向的边缘
那么我们需要提取不同的特征
怎么办
加多几种滤波器不就行了吗
对了
所以假设我们加到
种滤波器
每种滤波器的参数不一样
表示它提出输入图像的不同特征
例如不同的边缘
这样每种滤波器去卷积图像就得到对图像的不同特征的放映
我们称之为
是卷积之后的
还是加偏置之前的呢
应该是之后的吧
所以
种卷积核就有
个
这
个
就组成了一层神经元
到这个时候明了了吧
我们这一层有多少个参数了
种卷积核
每种卷积核共享
个参数
也就是
万个参数
才
万个参数啊
亲
又来了
受不了了
见下图右
不同的颜色表达不同的滤波器
嘿哟
遗漏一个问题了
刚才说隐层的参数个数和隐层的神经元个数无关
只和滤波器的大小和滤波器种类的多少有关
那么隐层的神经元个数
特征图上的
其实都是神经元
怎么确定呢
它和原图像
也就是输入的大小
神经元个数
滤波器的大小和滤波器在图像中的滑动步长都有关
例如
我的图像是
像素
而滤波器大小是
假设滤波器没有重叠
也就是步长为
这样隐层的神经元个数就是
个神经元了
假设步长是
也就是卷积核会重叠两个像素
那么
我就不算了
思想懂了就好
注意了
这只是一种滤波器
也就是一个
的神经元个数哦
如果
个
就是
倍了
由此可见
图像越大
神经元个数和需要训练的权值参数个数的贫富差距就越大
需要注意的一点是
上面的讨论都没有考虑每个神经元的偏置部分
所以权值个数需要加
这个也是同一种滤波器共享的
总之
卷积网络的核心思想是将
局部感受野
权值共享
或者权值复制
以及时间或空间亚采样这三种结构思想结合起来获得了某种程度的位移
尺度
形变不变性
一个典型的例子说明
一种典型的用来识别数字的卷积网络是
年
在论文
中提出了
并在字母识别中取得了很好的效果
当年美国大多数银行就是用它来识别支票上面的手写数字的
能够达到这种商用的地步
它的准确性可想而知
毕竟目前学术界和工业界的结合是最受争议的
那下面咱们也用这个例子来说明下
共有
层
不包含输入
每层都包含可训练参数
连接权重
输入图像为
大小
这要比
数据库
一个公认的手写数据库
中最大的字母还大
这样做的原因是希望潜在的明显特征如笔画断电或角点能够出现在最高层特征监测子感受野的中心
我们先要明确一点
每个层有多个
每个
通过一种卷积滤波器提取输入的一种特征
然后每个
有多个神经元
层是一个卷积层
为什么是卷积
卷积运算一个重要的特点就是
通过卷积运算
可以使原信号特征增强
并且降低噪音
由
个特征图
构成
特征图中每个神经元与输入中
的邻域相连
特征图的大小为
这样能防止输入的连接掉到边界之外
是为了
反馈时的计算
不致梯度损失
个人见解
有
个可训练参数
每个滤波器
个
参数和一个
参数
一共
个滤波器
共
个参数
共
个连接
层是一个下采样层
为什么是下采样
利用图像局部相关性的原理
对图像进行子抽样
可以减少数据处理量同时保留有用信息
有
个
的特征图
特征图中的每个单元与
中相对应特征图的
邻域相连接
层每个单元的
个输入相加
乘以一个可训练参数
再加上一个可训练偏置
结果通过
函数计算
可训练系数和偏置控制着
函数的非线性程度
如果系数比较小
那么运算近似于线性运算
亚采样相当于模糊图像
如果系数比较大
根据偏置的大小亚采样可以被看成是有噪声的
或
运算或者有噪声的
与
运算
每个单元的
感受野并不重叠
因此
中每个特征图的大小是
中特征图大小的
行和列各
层有
个可训练参数和
个连接
计算方式为
图
卷积和子采样过程
卷积过程包括
用一个可训练的滤波器
去卷积一个输入的图像
第一阶段是输入的图像
后面的阶段就是卷积特征
了
然后加一个偏置
得到卷积层
子采样过程包括
每邻域四个像素求和变为一个像素
然后通过标量
加权
再增加偏置
然后通过一个
激活函数
产生一个大概缩小四倍的特征映射图
所以从一个平面到下一个平面的映射可以看作是作卷积运算
层可看作是模糊滤波器
起到二次特征提取的作用
隐层与隐层之间空间分辨率递减
而每层所含的平面数递增
这样可用于检测更多的特征信息
层也是一个卷积层
它同样通过
的卷积核去卷积层
然后得到的特征
就只有
个神经元
但是它有
种不同的卷积核
所以就存在
个特征
了
这里需要注意的一点是
中的每个特征
是连接到
中的所有
个或者几个特征
下采样层生成的也是
的
表示本层的特征
是上一层提取到的特征
的不同组合
这个做法也并不是唯一的
看到没有
这里是组合
就像之前聊到的人的视觉系统一样
底层的结构构成上层更抽象的结构
例如边缘构成形状或者目标的部分
刚才说
中每个特征图由
中所有
个或者几个特征
组合而成
为什么不把
中的每个特征图连接到每个
的特征图呢
原因有
点
第一
不完全的连接机制将连接的数量保持在合理的范围内
第二
也是最重要的
其破坏了网络的对称性
由于不同的特征图有不同的输入
所以迫使他们抽取不同的特征
希望是互补的
例如
存在的一个方式是
的前
个特征图以
中
个相邻的特征图子集为输入
接下来
个特征图以
中
个相邻特征图子集为输入
然后的
个以不相邻的
个特征图子集为输入
最后一个将
中所有特征图为输入
这样
层有
计算方式
个可训练参数和
个连接
注意
对于一个后面卷积层生成的一张
只有一个统一的偏置
对前面一个
只用一个卷积核
所有前面
卷积之和再加上那个偏置的
层是一个下采样层
由
个
大小的特征图构成
特征图中的每个单元与
中相应特征图的
邻域相连接
跟
和
之间的连接一样
层有
个可训练参数
每个特征图
个因子和一个偏置
和
个连接
层是一个卷积层
有
个特征图
每个单元与
层的全部
个单元的
邻域相连
由于
层特征图的大小也为
同滤波器一样
故
特征图的大小为
这构成了
和
之间的全连接
之所以仍将
标示为卷积层而非全相联层
是因为如果
的输入变大
而其他的保持不变
那么此时特征图的维数就会比
大
层有
个可训练连接
层有
个单元
之所以选这个数字的原因来自于输出层的设计
与
层全相连
有
个可训练参数
如同经典神经网络
层计算输入向量和权重向量之间的点积
再加上一个偏置
然后将其传递给
函数产生单元
的一个状态
最后
输出层由欧式径向基函数
单元组成
每类一个单元
每个有
个输入
换句话说
每个输出
单元计算输入向量和参数向量之间的欧式距离
输入离参数向量越远
输出的越大
一个
输出可以被理解为衡量输入模式和与
相关联类的一个模型的匹配程度的惩罚项
用概率术语来说
输出可以被理解为
层配置空间的高斯分布的负
给定一个输入模式
损失函数应能使得
的配置与
参数向量
即模式的期望分类
足够接近
这些单元的参数是人工选取并保持固定的
至少初始时候如此
这些参数向量的成分被设为
或
虽然这些参数可以以
和
等概率的方式任选
或者构成一个纠错码
但是被设计成一个相应字符类的
大小
即
的格式化图片
这种表示对识别单独的数字不是很有用
但是对识别可打印
集中的字符串很有用
训练过程
神经网络用于模式识别的主流是有指导学习网络
无指导学习网络更多的是用于聚类分析
对于有指导的模式识别
由于任一样本的类别是已知的
样本在空间的分布不再是依据其自然分布倾向来划分
而是要根据同类样本在空间的分布及不同类样本之间的分离程度找一种适当的空间划分方法
或者找到一个分类边界
使得不同类样本分别位于不同的区域内
这就需要一个长时间且复杂的学习过程
不断调整用以划分样本空间的分类边界的位置
使尽可能少的样本被划分到非同类区域中
卷积网络在本质上是一种输入到输出的映射
它能够学习大量的输入与输出之间的映射关系
而不需要任何输入和输出之间的精确的数学表达式
只要用已知的模式对卷积网络加以训练
网络就具有输入输出对之间的映射能力
卷积网络执行的是有导师训练
所以其样本集是由形如
输入向量
理想输出向量
的向量对构成的
所有这些向量对
都应该是来源于网络即将模拟的系统的实际
运行
结果
它们可以是从实际运行系统中采集来的
在开始训练前
所有的权都应该用一些不同的小随机数进行初始化
小随机数
用来保证网络不会因权值过大而进入饱和状态
从而导致训练失败
不同
用来保证网络可以正常地学习
实际上
如果用相同的数去初始化权矩阵
则网络无能力学习
训练算法与传统的
算法差不多
主要包括
步
这
步被分为两个阶段
第一阶段
向前传播阶段
从样本集中取一个样本
将
输入网络
计算相应的实际输出
在此阶段
信息从输入层经过逐级的变换
传送到输出层
这个过程也是网络在完成训练后正常运行时执行的过程
在此过程中
网络执行的是计算
实际上就是输入与每层的权值矩阵相点乘
得到最后的输出结果
第二阶段
向后传播阶段
算实际输出
与相应的理想输出
的差
按极小化误差的方法反向传播调整权矩阵
里面还有
的参数配置的文件
经典模型整理
大层
文章
下图是广为流传
的网络结构
麻雀虽小
但五脏俱全
卷积层
层
全连接层
这些都是现代
网络的基本组件
可以看下
中
的配置文件
点我
可以试着理解每一层的大小
和各种参数
由两个卷积层
两个池化层
以及两个全连接层组成
卷积都是
的模板
池化都是
上图是一个类似的结构
可以帮助理解层次结构
和
不完全一致
不过基本上差不多
输入尺寸
卷积层
个
降采样层
个
全连接层
个
输出
个类别
数字
的概率
输入图像
为
这要比
数据库
中最大的字母
还大
这样做的目的是希望潜在的明显特征
如笔画断续
角点能够出现在最高层特征监测子感受野的中心
卷积层
卷积核在二维平面上平移
并且卷积核的每个元素与被卷积图像对应位置相乘
再求和
通过卷积核的不断移动
我们就有了一个新的图像
这个图像完全由卷积核在各个位置时的乘积求和的结果组成
二维卷积在图像中的效果就是
对图像的每个像素的邻域
邻域大小就是核的大小
加权求和得到该像素点的输出值
具体做法如下
卷积运算一个重要的特点就是
通过卷积运算
可以使原信号特征增强
并且降低噪音
层
文章
非常好
年
比赛冠军的
以第一作者
命名
的
文件在这里
说实话
这个
的意义比后面那些
都大很多
首先它证明了
在复杂模型下的有效性
然后
实现使得训练在可接受的时间范围内得到结果
确实让
和
都大火了一把
顺便推动了有监督
的发展
之所以能够成功
深度学习之所以能够重回历史舞台
原因在于
非线性激活函数
防止过拟合的方法
大数据训练
百万级
图像数据
其他
实现
归一化层的使用
下面简单介绍一下
的一些细节
有一种观点认为神经网络是靠数据喂出来的
若增加训练数据
则能够提升
算法
的准确率
因为这样可以避免过拟合
而避免了过拟合你就可以增大你的网络结构了
当训练数据有限的时候
可以通过一些变换来从已有的训练数据集中生成一些新的数据
来扩大训练数据的
其中
最简单
通用的图像数据变形的方式
从原始图像
中
随机的
出一些图像
平移变换
水平翻转图像
反射变换
给图像增加一些随机的光照
光照
彩色变换
训练的时候
在
上处理的很好
随机
训练时候
对于
的图片进行随机
到
然后允许水平翻转
那么相当与将样本倍增到
测试时候
对左上
右上
左下
右下
中间做了
次
然后翻转
共
个
之后对结果求平均
作者说
不做随机
大网络基本都过拟合
对
空间做
然后对主成分做一个
的高斯扰动
结果让错误率又下降了
激活函数
是常用的非线性的激活函数
它能够把输入的连续实值
压缩
到
和
之间
特别的
如果是非常大的负数
那么输出就是
如果是非常大的正数
输出就是
但是它有一些致命的
缺点
有一个非常致命的缺点
当输入非常大或者非常小的时候
会有饱和现象
这些神经元的梯度是接近于
的
如果你的初始值很大的话
梯度在反向传播的时候因为需要乘上一个
的导数
所以会使得梯度越来越小
这会导致网络变的很难学习
的
不是
均值
这是不可取的
因为这会导致后一层的神经元将得到上一层输出的非
均值的信号作为输入
产生的一个结果就是
如果数据进入神经元的时候是正的
那么
计算出的梯度也会始终都是正的
当然了
如果你是按
去训练
那么那个
可能得到不同的信号
所以这个问题还是可以缓解一下的
因此
非
均值这个问题虽然会产生一些不好的影响
不过跟上面提到的
问题相比还是要好很多的
的数学表达式
很显然
从图左可以看出
输入信号
时
输出都是
的情况下
输出等于输入
是二维的情况下
使用
之后的效果如下
用
代替了
发现使用
得到的
的收敛速度会比
快很多
主要是因为它是
而且
因为
的导数始终是
相比于
只需要一个阈值就可以得到激活值
而不用去算一大堆复杂的运算
结合预先训练好的许多不同模型
来进行预测是一种非常成功的减少测试误差的方式
但因为每个模型的训练都需要花了好几天时间
因此这种做法对于大型神经网络来说太过昂贵
然而
提出了一个非常有效的模型组合版本
它在训练中只需要花费两倍于单模型的时间
这种技术叫做
它做的就是以
的概率
将每个隐层神经元的输出设置为零
以这种方式
的神经元既不参与前向传播
也不参与反向传播
所以每次输入一个样本
就相当于该神经网络就尝试了一个新的结构
但是所有这些结构之间共享权重
因为神经元不能依赖于其他特定神经元而存在
所以这种技术降低了神经元复杂的互适应关系
正因如此
网络需要被迫学习更为鲁棒的特征
这些特征在结合其他神经元的一些不同随机子集时有用
在测试时
我们将所有神经元的输出都仅仅只乘以
对于获取指数级
网络产生的预测分布的几何平均值
这是一个合理的近似方法
这边似懂非懂
估计有待于看代码
它做的事是对当前层的输出结果做平滑处理
下面是我画的示意图
前后几层
对应位置的点
对中间这一层做一下平滑约束
计算方法是
一句话概括
本质上
这个层也是为了防止激活函数的饱和的
个人理解原理是通过正则化让激活函数的输入靠近
碗
的中间
避免饱和
从而获得比较大的导数值
所以从功能上说
跟
是重复的
不过作者说
从试验结果看
操作可以提高网络的泛化能力
将错误率降低了大约
个百分点
优势在于
网络增大
个卷积层
个全连接层
个
层
同时解决过拟合
并且利用多
加速计算
模型结构
见下图
别看只有寥寥八层
不算
层
这里是把卷积层到
层放一起
认为是一层
认为是这层卷积层到下一层卷积层统一是一层
对于层的概念好像有点模糊
各处可能会不一样吧
但是它有
以上的参数总量
事实上在参数量上比后面的网络都大
这个图有点点特殊的地方是卷积部分都是画成上下两块
是用两块
并行计算的
意思是说吧这一层计算出来的
分开
但是前一层
意思是后面一层
用到的数据要看连接的虚线
如图中
层之后的第一层第二层之间的虚线是分开的
是说二层上面的
是由一层上面的
计算的
下面同理
而第三层前面的虚线是完全交叉的
就是说每一个
都是由前面的
同时计算得到的
具体打开
的每一阶段来看
具体计算都在图里面写了
要注意的是
层是
而不是
里面的
这里可以算一下
主要是
可以整除后面的
计算
不整除
如果一定要用
可以通过自动补边实现
不过在
就补边感觉没有意义
补得也是
和上面基本一样
唯独需要注意的是
这个属性强行把前面结果的
分开
卷积部分分成两部分做
这里有一层特殊的
层
在
中是说在训练的以
概率使得隐藏层的某些
的输出为
这样就丢到了一半节点的输出
的时候也不更新这些节点
层
文章
年比赛冠军的
这个
证明了一件事
用更多的卷积
更深的层次可以得到更好的结构
当然
它并没有证明浅的层次不能达到这样的效果
这个
基本上构成部件和
差不多
不过中间有好几个
的结构
是说一分四
然后做一些不同大小的卷积
之后再堆叠
第三层开始时
这个的思想受到使用不同尺度的
过滤器来处理多尺度问题
采用不同尺度的卷积核来处理问题
包含
四个支线
个
的卷积核
之后进行
计算
变成
个
的卷积核
作为
卷积核之前的
变成
进行
计算后
再进行
个
的卷积
为
个
的卷积核
作为
卷积核之前的
变成
进行
计算后
再进行
个
的卷积
为
变成
层
的核
为
输出还是
然后进行
个
的卷积
变成
将四个结果进行连接
输出为
然后将
的结果又分成四条支线
开始建立
的
个
的卷积核
之后进行
计算
变成
个
的卷积核
作为
卷积核之前的
变成
再进行
个
的卷积
为
进行
计算
个
的卷积核
作为
卷积核之前的
变成
进行
计算后
再进行
个
的卷积
为
变成
层
的核
为
输出还是
然后进行
个
的卷积
变成
将四个结果进行连接
输出为
计算量如下图
可以看到参数总量并不大
但是计算次数是非常大的
层
文章
在
上定位第一
分类第二
有很多个版本
也算是比较稳定和经典的
它的特点也是连续
多
计算量巨大
比前面几个都大很多
具体的
结构可以参考
这里给一个简图
基本上组成构建就是前面
用到的
和
是
年
竞赛的双雄
这两类模型结构有一个共同特点是
跟
不同的是
继承了
以及
的一些框架
尤其是跟
框架非常像
也是
个
的卷积
层
图像特征
一层
分类特征
可以看做和
一样总共
个
根据前
个卷积
每个
中的不同配置
论文中给出了
这五种配置
卷积层数从
到
递增
从论文中可以看到从
到
随着卷积层的一步步加深
貌似通过加深卷积层数也已经到达准确率提升的瓶颈了
下面是几个
的具体结构
可以查阅
很容易看懂
文章
这个
是
年底最新给出的
也是
年的
比赛冠军
可以说是进一步将
进行到底
其特殊之处在于设计了
形式的
有跨越几层的直连
最深的
采用的
层
下面是一个
层的例子
更深的
见表格
其实这个
构成上更加简单
连
这样的
都没有了
的构成见下图
总结
到这里把常见的最新的几个
都介绍完了
可以看到
目前
的设计思路基本上朝着深度的网络以及更多的卷积计算方向发展
虽然有点暴力
但是效果上确实是提升了
当然
我认为以后会出现更优秀的
方向应该不是更深
而是简化
是时候动一动卷积计算的形式了
之前的理解
卷积层做的包含非线性激活
层做的就是激活之后的
中值的平均或者最大值等等
现在貌似各个网络的设计先后都会不一样
著作权归作者所有
举报文章
每天进步一点点
下载
生成长微博图片
更多分享
李理
详解卷积神经网络
本系列文章面向深度学习研发者
希望通过
一个有意思的具体任务
深入浅出地介绍深度学习的知识
本系列文章涉及到很多深度学习流行的模型
如
等
本文为第
篇
作者
李理
目前就职于环信
课程笔记五
卷积神经网络
卷积神经网络类似于一般的神经网络
由可学习的权重和误差组成
每一个神经元接受一些输入
完成一些非线性的操作
整个神经网络完成了一个可微的打分函数
从图像点到分类得分
在全连接或者最后一层他们也有一个损失函数
而卷积神经网络中有明确的假设那就是输入时图像
这代表了网络机构具有
彭岩
李理
自动梯度求解
使用自动求导实现多层神经网络
本系列文章面向深度学习研发者
希望通过
一个有意思的具体任务
深入浅出地介绍深度学习的知识
本系列文章涉及到很多深度学习流行的模型
如
等
本文为第
篇
作者
李理
目前就职于环信
卷积神经网络的通俗解释和经典模型介绍
神经网络是在传统多项式回归的基础上
受到了生物神经网络
激活
现象的启发
引入了激活函数而构建起来的机器学习模型
在图像处理领域
由于图像的数据量非常大
伴随着产生的问题是网络参数量非常大
而卷积神经网络引入卷积核巧妙地优化了这个问题
卷积核对图像进行局部扫描
提取其中的特
扣带回
卷积神经网络全面生动解读
深度学习笔记
原文地址
更多机器学习
深度学习笔记
含
代码实现
知行
道非道
去年
月的一个周末
我在公园偶遇文松
聊天的时候说起好久没有见到以前的老同事
不知大家近况如何
文松顿了顿说自从公司从私企变为合资
每天除了开会就是开会
想聚一聚都脱不开身
月的一天
我在那栋曾经很熟悉的写字楼前又遇见文松
聊了几句他便匆忙上楼开会了
月
我再次
肥帝
记感慨颇多的早晨
出门骑个车也被虐狗
情绪
终于要流露
给过去的青春
久久干涩的眼眶也溢出泪珠
谢谢你
那个拥抱
我记得
随笔
看了一篇报纸上的报道
很有意思
像许三多那样露齿而笑
我们想把他当作自己的兄弟
那个一笑就露出满口大白牙的士兵
岁末
年终盘点的时候
许三多
突然成为众多新闻类周刊杂志的封面先生
士兵突击
一部没有女主角的纯男人戏
从夏天一直火到冬天
王宝强终于不用被叫做
傻
心无空城
三八节
是个说爱的节日
今年十一岁
女儿
她最喜欢对我说的一句话是
妈妈
我真的好爱你
她喜欢每个节日里
亲手绘制小卡片
喜欢做完作业
跑来让我抱一抱
三八节
女儿给我和外婆给各做了一张贺卡
给爸爸发了一条微信
说爸爸
父女节快乐
我跟先生说
女儿比我们更懂得爱的含义和表达
我们常常以为自
婷子
曦
摄影